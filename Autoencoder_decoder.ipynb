{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "colab": {
      "name": "Autoencoder-decoder.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haiyennghy/Anomaly-Detection/blob/master/Autoencoder_decoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xNSXzLwBM8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# importing utilities\n",
        "import os\n",
        "import sys\n",
        "from datetime import datetime\n",
        "\n",
        "# importing data science libraries\n",
        "import pandas as pd\n",
        "import random as rd\n",
        "import numpy as np\n",
        "\n",
        "# importing pytorch libraries\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch import autograd\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# import visualization libraries\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import seaborn as sns\n",
        "from IPython.display import Image, display\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "# ignore potential warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgHz5Vc3BM80",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "USE_CUDA = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A-qvORPBM83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init deterministic seed\n",
        "seed_value = 1234 #4444 #3333 #2222 #1111 #1234\n",
        "rd.seed(seed_value) # set random seed\n",
        "np.random.seed(seed_value) # set numpy seed\n",
        "torch.manual_seed(seed_value) # set pytorch seed CPU\n",
        "if (torch.backends.cudnn.version() != None and USE_CUDA == True):\n",
        "    torch.cuda.manual_seed(seed_value) # set pytorch seed GPU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tO2VooaVBM87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# load the dataset into the notebook kernel\n",
        "ori_dataset = pd.read_csv('https://raw.githubusercontent.com/haiyennghy/Anomaly-Detection/master/train_200k.csv')\n",
        "test_data = pd.read_csv('https://raw.githubusercontent.com/haiyennghy/Anomaly-Detection/master/test_20k.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ZEOlRY7BM8-",
        "colab_type": "code",
        "outputId": "1ec51874-854e-4d41-c128-556bce82a7ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# inspect top rows of dataset\n",
        "ori_dataset.head(10) "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>BELNR</th>\n",
              "      <th>WAERS</th>\n",
              "      <th>BUKRS</th>\n",
              "      <th>KTOSL</th>\n",
              "      <th>PRCTR</th>\n",
              "      <th>BSCHL</th>\n",
              "      <th>HKONT</th>\n",
              "      <th>DMBTR</th>\n",
              "      <th>WRBTR</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>288203</td>\n",
              "      <td>C3</td>\n",
              "      <td>C31</td>\n",
              "      <td>C9</td>\n",
              "      <td>C92</td>\n",
              "      <td>A3</td>\n",
              "      <td>B1</td>\n",
              "      <td>280979.60</td>\n",
              "      <td>0.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>324441</td>\n",
              "      <td>C1</td>\n",
              "      <td>C18</td>\n",
              "      <td>C7</td>\n",
              "      <td>C76</td>\n",
              "      <td>A1</td>\n",
              "      <td>B2</td>\n",
              "      <td>129856.53</td>\n",
              "      <td>243343.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>133537</td>\n",
              "      <td>C1</td>\n",
              "      <td>C19</td>\n",
              "      <td>C2</td>\n",
              "      <td>C20</td>\n",
              "      <td>A1</td>\n",
              "      <td>B3</td>\n",
              "      <td>957463.97</td>\n",
              "      <td>3183838.41</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>331521</td>\n",
              "      <td>C4</td>\n",
              "      <td>C48</td>\n",
              "      <td>C9</td>\n",
              "      <td>C95</td>\n",
              "      <td>A2</td>\n",
              "      <td>B1</td>\n",
              "      <td>2681709.51</td>\n",
              "      <td>28778.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>375333</td>\n",
              "      <td>C5</td>\n",
              "      <td>C58</td>\n",
              "      <td>C1</td>\n",
              "      <td>C19</td>\n",
              "      <td>A3</td>\n",
              "      <td>B1</td>\n",
              "      <td>910514.49</td>\n",
              "      <td>346.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>327203</td>\n",
              "      <td>C1</td>\n",
              "      <td>C15</td>\n",
              "      <td>C6</td>\n",
              "      <td>C68</td>\n",
              "      <td>A1</td>\n",
              "      <td>B2</td>\n",
              "      <td>357627.56</td>\n",
              "      <td>704520.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>292545</td>\n",
              "      <td>C4</td>\n",
              "      <td>C47</td>\n",
              "      <td>C2</td>\n",
              "      <td>C28</td>\n",
              "      <td>A2</td>\n",
              "      <td>B3</td>\n",
              "      <td>955576.84</td>\n",
              "      <td>128328.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>335839</td>\n",
              "      <td>C1</td>\n",
              "      <td>C19</td>\n",
              "      <td>C1</td>\n",
              "      <td>C17</td>\n",
              "      <td>A1</td>\n",
              "      <td>B1</td>\n",
              "      <td>41769.26</td>\n",
              "      <td>0.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>369064</td>\n",
              "      <td>C4</td>\n",
              "      <td>C40</td>\n",
              "      <td>C9</td>\n",
              "      <td>C97</td>\n",
              "      <td>A2</td>\n",
              "      <td>B1</td>\n",
              "      <td>44309.79</td>\n",
              "      <td>0.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>138724</td>\n",
              "      <td>C6</td>\n",
              "      <td>C69</td>\n",
              "      <td>C1</td>\n",
              "      <td>C12</td>\n",
              "      <td>A2</td>\n",
              "      <td>B1</td>\n",
              "      <td>466720.45</td>\n",
              "      <td>43843.00</td>\n",
              "      <td>regular</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    BELNR WAERS BUKRS KTOSL PRCTR BSCHL HKONT       DMBTR       WRBTR    label\n",
              "0  288203    C3   C31    C9   C92    A3    B1   280979.60        0.00  regular\n",
              "1  324441    C1   C18    C7   C76    A1    B2   129856.53   243343.00  regular\n",
              "2  133537    C1   C19    C2   C20    A1    B3   957463.97  3183838.41  regular\n",
              "3  331521    C4   C48    C9   C95    A2    B1  2681709.51    28778.00  regular\n",
              "4  375333    C5   C58    C1   C19    A3    B1   910514.49      346.00  regular\n",
              "5  327203    C1   C15    C6   C68    A1    B2   357627.56   704520.00  regular\n",
              "6  292545    C4   C47    C2   C28    A2    B3   955576.84   128328.00  regular\n",
              "7  335839    C1   C19    C1   C17    A1    B1    41769.26        0.00  regular\n",
              "8  369064    C4   C40    C9   C97    A2    B1    44309.79        0.00  regular\n",
              "9  138724    C6   C69    C1   C12    A2    B1   466720.45    43843.00  regular"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENv3KnCPBM9C",
        "colab_type": "code",
        "outputId": "9a81ce69-d87b-494b-e053-755e623ff10c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# number of anomalies vs. regular transactions\n",
        "ori_dataset.label.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "regular    100000\n",
              "global      50000\n",
              "local       50000\n",
              "Name: label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUekDUa2BM9G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove the \"ground-truth\" label information for the following steps of the lab\n",
        "label = ori_dataset.pop('label')\n",
        "test_label = test_data.pop('label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "goUQKpq_BM9L",
        "colab_type": "code",
        "outputId": "f2ee04ad-dd4f-4eef-86db-85eb3ed72672",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# select categorical attributes to be \"one-hot\" encoded\n",
        "categorical_attr_names = ['WAERS','BUKRS', 'KTOSL', 'PRCTR', 'BSCHL', 'HKONT']\n",
        "\n",
        "full_data = pd.concat([ori_dataset,test_data], keys=[0,1])\n",
        "temp = pd.get_dummies(full_data[categorical_attr_names])\n",
        "\n",
        "# encode categorical attributes into a binary one-hot encoded representation \n",
        "# ori_dataset_categ_transformed = pd.get_dummies(ori_dataset[categorical_attr_names])\n",
        "ori_dataset_categ_transformed, test_dataset_categ_transformed = temp.xs(0), temp.xs(1)\n",
        "print(ori_dataset_categ_transformed.shape)\n",
        "print(test_dataset_categ_transformed.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 13704)\n",
            "(20000, 13704)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gGhm9dPhBM9P",
        "colab_type": "code",
        "outputId": "63df5d4c-c583-4efa-ced3-40cae2e0a5ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 405
        }
      },
      "source": [
        "# inspect encoded sample transactions\n",
        "ori_dataset_categ_transformed.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>WAERS_A10</th>\n",
              "      <th>WAERS_A11</th>\n",
              "      <th>WAERS_A12</th>\n",
              "      <th>WAERS_A13</th>\n",
              "      <th>WAERS_A14</th>\n",
              "      <th>WAERS_A15</th>\n",
              "      <th>WAERS_A16</th>\n",
              "      <th>WAERS_A17</th>\n",
              "      <th>WAERS_A18</th>\n",
              "      <th>WAERS_A19</th>\n",
              "      <th>WAERS_A20</th>\n",
              "      <th>WAERS_A21</th>\n",
              "      <th>WAERS_A22</th>\n",
              "      <th>WAERS_A23</th>\n",
              "      <th>WAERS_A24</th>\n",
              "      <th>WAERS_A25</th>\n",
              "      <th>WAERS_A26</th>\n",
              "      <th>WAERS_A27</th>\n",
              "      <th>WAERS_A28</th>\n",
              "      <th>WAERS_A29</th>\n",
              "      <th>WAERS_A30</th>\n",
              "      <th>WAERS_A31</th>\n",
              "      <th>WAERS_A32</th>\n",
              "      <th>WAERS_A33</th>\n",
              "      <th>WAERS_A34</th>\n",
              "      <th>WAERS_A35</th>\n",
              "      <th>WAERS_A36</th>\n",
              "      <th>WAERS_A37</th>\n",
              "      <th>WAERS_A38</th>\n",
              "      <th>WAERS_A39</th>\n",
              "      <th>WAERS_A40</th>\n",
              "      <th>WAERS_A41</th>\n",
              "      <th>WAERS_A42</th>\n",
              "      <th>WAERS_A43</th>\n",
              "      <th>WAERS_A44</th>\n",
              "      <th>WAERS_A45</th>\n",
              "      <th>WAERS_A46</th>\n",
              "      <th>WAERS_A47</th>\n",
              "      <th>WAERS_A48</th>\n",
              "      <th>WAERS_A49</th>\n",
              "      <th>...</th>\n",
              "      <th>HKONT_Z60</th>\n",
              "      <th>HKONT_Z61</th>\n",
              "      <th>HKONT_Z62</th>\n",
              "      <th>HKONT_Z63</th>\n",
              "      <th>HKONT_Z64</th>\n",
              "      <th>HKONT_Z65</th>\n",
              "      <th>HKONT_Z66</th>\n",
              "      <th>HKONT_Z67</th>\n",
              "      <th>HKONT_Z68</th>\n",
              "      <th>HKONT_Z69</th>\n",
              "      <th>HKONT_Z70</th>\n",
              "      <th>HKONT_Z71</th>\n",
              "      <th>HKONT_Z72</th>\n",
              "      <th>HKONT_Z73</th>\n",
              "      <th>HKONT_Z74</th>\n",
              "      <th>HKONT_Z75</th>\n",
              "      <th>HKONT_Z76</th>\n",
              "      <th>HKONT_Z77</th>\n",
              "      <th>HKONT_Z78</th>\n",
              "      <th>HKONT_Z79</th>\n",
              "      <th>HKONT_Z80</th>\n",
              "      <th>HKONT_Z81</th>\n",
              "      <th>HKONT_Z82</th>\n",
              "      <th>HKONT_Z83</th>\n",
              "      <th>HKONT_Z84</th>\n",
              "      <th>HKONT_Z85</th>\n",
              "      <th>HKONT_Z86</th>\n",
              "      <th>HKONT_Z87</th>\n",
              "      <th>HKONT_Z88</th>\n",
              "      <th>HKONT_Z89</th>\n",
              "      <th>HKONT_Z90</th>\n",
              "      <th>HKONT_Z91</th>\n",
              "      <th>HKONT_Z92</th>\n",
              "      <th>HKONT_Z93</th>\n",
              "      <th>HKONT_Z94</th>\n",
              "      <th>HKONT_Z95</th>\n",
              "      <th>HKONT_Z96</th>\n",
              "      <th>HKONT_Z97</th>\n",
              "      <th>HKONT_Z98</th>\n",
              "      <th>HKONT_Z99</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 13704 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   WAERS_A10  WAERS_A11  WAERS_A12  ...  HKONT_Z97  HKONT_Z98  HKONT_Z99\n",
              "0          0          0          0  ...          0          0          0\n",
              "1          0          0          0  ...          0          0          0\n",
              "2          0          0          0  ...          0          0          0\n",
              "3          0          0          0  ...          0          0          0\n",
              "4          0          0          0  ...          0          0          0\n",
              "5          0          0          0  ...          0          0          0\n",
              "6          0          0          0  ...          0          0          0\n",
              "7          0          0          0  ...          0          0          0\n",
              "8          0          0          0  ...          0          0          0\n",
              "9          0          0          0  ...          0          0          0\n",
              "\n",
              "[10 rows x 13704 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Em00O2EoBM9U",
        "colab_type": "code",
        "outputId": "8dad68a4-5702-4303-8dca-92801dddb094",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# select \"DMBTR\" vs. \"WRBTR\" attribute\n",
        "numeric_attr_names = ['DMBTR', 'WRBTR']\n",
        "\n",
        "# add a small epsilon to eliminate zero values from data for log scaling\n",
        "numeric_attr = full_data[numeric_attr_names] + 1e-7\n",
        "numeric_attr = numeric_attr.apply(np.log)\n",
        "\n",
        "# normalize all numeric attributes to the range [0,1]\n",
        "# ori_dataset_numeric_attr = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
        "temp = (numeric_attr - numeric_attr.min()) / (numeric_attr.max() - numeric_attr.min())\n",
        "ori_dataset_numeric_attr, test_dataset_numeric_attr = temp.xs(0), temp.xs(1)\n",
        "print(ori_dataset_numeric_attr.shape)\n",
        "print(test_dataset_numeric_attr.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 2)\n",
            "(20000, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89Yf9BzdBM9Y",
        "colab_type": "code",
        "outputId": "db660368-6f77-4aec-cc4d-c995ce293337",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# merge categorical and numeric subsets\n",
        "ori_subset_transformed = pd.concat([ori_dataset_categ_transformed, ori_dataset_numeric_attr], axis = 1)\n",
        "test_subset_transformed = pd.concat([test_dataset_categ_transformed, test_dataset_numeric_attr], axis = 1)\n",
        "print(ori_subset_transformed.shape)\n",
        "print(test_subset_transformed.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(200000, 13706)\n",
            "(20000, 13706)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xv8_QP2uBM9d",
        "colab_type": "code",
        "outputId": "84752fc7-7a4e-4bf0-b570-ed0460b3ce78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# inspect final dimensions of pre-processed transactional data\n",
        "ori_subset_transformed.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(200000, 13706)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t911BTpjBM9h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation of the encoder network\n",
        "class encoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(encoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 618, out 512\n",
        "        self.encoder_L1 = nn.Linear(in_features=ori_subset_transformed.shape[1], out_features=512, bias=True) # add linearity \n",
        "        nn.init.xavier_uniform_(self.encoder_L1.weight) # init weights according to [9]\n",
        "        self.encoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 512, out 256\n",
        "        self.encoder_L2 = nn.Linear(512, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L2.weight)\n",
        "        self.encoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 256, out 128\n",
        "        self.encoder_L3 = nn.Linear(256, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L3.weight)\n",
        "        self.encoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 128, out 64\n",
        "        self.encoder_L4 = nn.Linear(128, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L4.weight)\n",
        "        self.encoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 64, out 32\n",
        "        self.encoder_L5 = nn.Linear(64, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L5.weight)\n",
        "        self.encoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 32, out 16\n",
        "        self.encoder_L6 = nn.Linear(32, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L6.weight)\n",
        "        self.encoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 7 - in 16, out 8\n",
        "        self.encoder_L7 = nn.Linear(16, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L7.weight)\n",
        "        self.encoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 8, out 4\n",
        "        self.encoder_L8 = nn.Linear(8, 4, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L8.weight)\n",
        "        self.encoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 4, out 3\n",
        "        self.encoder_L9 = nn.Linear(4, 3, bias=True)\n",
        "        nn.init.xavier_uniform_(self.encoder_L9.weight)\n",
        "        self.encoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "        \n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.encoder_R1(self.dropout(self.encoder_L1(x)))\n",
        "        x = self.encoder_R2(self.dropout(self.encoder_L2(x)))\n",
        "        x = self.encoder_R3(self.dropout(self.encoder_L3(x)))\n",
        "        x = self.encoder_R4(self.dropout(self.encoder_L4(x)))\n",
        "        x = self.encoder_R5(self.dropout(self.encoder_L5(x)))\n",
        "        x = self.encoder_R6(self.dropout(self.encoder_L6(x)))\n",
        "        x = self.encoder_R7(self.dropout(self.encoder_L7(x)))\n",
        "        x = self.encoder_R8(self.dropout(self.encoder_L8(x)))\n",
        "        x = self.encoder_R9(self.encoder_L9(x)) # don't apply dropout to the AE bottleneck\n",
        "\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZwM-eQ2BM9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init training network classes / architectures\n",
        "encoder_train = encoder()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3WgMPsABM9o",
        "colab_type": "code",
        "outputId": "1be237dd-5b41-4532-84cd-f9d80967b9d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# print the initialized architectures\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print('[LOG {}] encoder architecture:\\n\\n{}\\n'.format(now, encoder_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LOG 20191130-10:43:19] encoder architecture:\n",
            "\n",
            "encoder(\n",
            "  (encoder_L1): Linear(in_features=13706, out_features=512, bias=True)\n",
            "  (encoder_R1): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L2): Linear(in_features=512, out_features=256, bias=True)\n",
            "  (encoder_R2): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L3): Linear(in_features=256, out_features=128, bias=True)\n",
            "  (encoder_R3): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L4): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (encoder_R4): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L5): Linear(in_features=64, out_features=32, bias=True)\n",
            "  (encoder_R5): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L6): Linear(in_features=32, out_features=16, bias=True)\n",
            "  (encoder_R6): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L7): Linear(in_features=16, out_features=8, bias=True)\n",
            "  (encoder_R7): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L8): Linear(in_features=8, out_features=4, bias=True)\n",
            "  (encoder_R8): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (encoder_L9): Linear(in_features=4, out_features=3, bias=True)\n",
            "  (encoder_R9): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=True)\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6_YL3OQBM9r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# implementation of the decoder network\n",
        "class decoder(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super(decoder, self).__init__()\n",
        "\n",
        "        # specify layer 1 - in 3, out 4\n",
        "        self.decoder_L1 = nn.Linear(in_features=3, out_features=4, bias=True) # add linearity \n",
        "        nn.init.xavier_uniform_(self.decoder_L1.weight)  # init weights according to [9]\n",
        "        self.decoder_R1 = nn.LeakyReLU(negative_slope=0.4, inplace=True) # add non-linearity according to [10]\n",
        "\n",
        "        # specify layer 2 - in 4, out 8\n",
        "        self.decoder_L2 = nn.Linear(4, 8, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L2.weight)\n",
        "        self.decoder_R2 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 3 - in 8, out 16\n",
        "        self.decoder_L3 = nn.Linear(8, 16, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L3.weight)\n",
        "        self.decoder_R3 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 4 - in 16, out 32\n",
        "        self.decoder_L4 = nn.Linear(16, 32, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L4.weight)\n",
        "        self.decoder_R4 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 5 - in 32, out 64\n",
        "        self.decoder_L5 = nn.Linear(32, 64, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L5.weight)\n",
        "        self.decoder_R5 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 6 - in 64, out 128\n",
        "        self.decoder_L6 = nn.Linear(64, 128, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L6.weight)\n",
        "        self.decoder_R6 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "        \n",
        "        # specify layer 7 - in 128, out 256\n",
        "        self.decoder_L7 = nn.Linear(128, 256, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L7.weight)\n",
        "        self.decoder_R7 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 8 - in 256, out 512\n",
        "        self.decoder_L8 = nn.Linear(256, 512, bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L8.weight)\n",
        "        self.decoder_R8 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # specify layer 9 - in 512, out 618\n",
        "        self.decoder_L9 = nn.Linear(in_features=512, out_features=ori_subset_transformed.shape[1], bias=True)\n",
        "        nn.init.xavier_uniform_(self.decoder_L9.weight)\n",
        "        self.decoder_R9 = nn.LeakyReLU(negative_slope=0.4, inplace=True)\n",
        "\n",
        "        # init dropout layer with probability p\n",
        "        self.dropout = nn.Dropout(p=0.0, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # define forward pass through the network\n",
        "        x = self.decoder_R1(self.dropout(self.decoder_L1(x)))\n",
        "        x = self.decoder_R2(self.dropout(self.decoder_L2(x)))\n",
        "        x = self.decoder_R3(self.dropout(self.decoder_L3(x)))\n",
        "        x = self.decoder_R4(self.dropout(self.decoder_L4(x)))\n",
        "        x = self.decoder_R5(self.dropout(self.decoder_L5(x)))\n",
        "        x = self.decoder_R6(self.dropout(self.decoder_L6(x)))\n",
        "        x = self.decoder_R7(self.dropout(self.decoder_L7(x)))\n",
        "        x = self.decoder_R8(self.dropout(self.decoder_L8(x)))\n",
        "        x = self.decoder_R9(self.decoder_L9(x)) # don't apply dropout to the AE output\n",
        "        \n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9JP8am9BM9v",
        "colab_type": "code",
        "outputId": "888c326f-96b9-43c6-c622-1ec2cce5d38d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        }
      },
      "source": [
        "# init training network classes / architectures\n",
        "decoder_train = decoder()\n",
        "\n",
        "# push to cuda if cudnn is available\n",
        "if (torch.backends.cudnn.version() != None) and (USE_CUDA == True):\n",
        "    decoder_train = decoder().cuda()\n",
        "    \n",
        "# print the initialized architectures\n",
        "now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "print('[LOG {}] decoder architecture:\\n\\n{}\\n'.format(now, decoder_train))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LOG 20191130-10:43:30] decoder architecture:\n",
            "\n",
            "decoder(\n",
            "  (decoder_L1): Linear(in_features=3, out_features=4, bias=True)\n",
            "  (decoder_R1): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L2): Linear(in_features=4, out_features=8, bias=True)\n",
            "  (decoder_R2): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L3): Linear(in_features=8, out_features=16, bias=True)\n",
            "  (decoder_R3): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L4): Linear(in_features=16, out_features=32, bias=True)\n",
            "  (decoder_R4): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L5): Linear(in_features=32, out_features=64, bias=True)\n",
            "  (decoder_R5): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L6): Linear(in_features=64, out_features=128, bias=True)\n",
            "  (decoder_R6): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L7): Linear(in_features=128, out_features=256, bias=True)\n",
            "  (decoder_R7): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L8): Linear(in_features=256, out_features=512, bias=True)\n",
            "  (decoder_R8): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (decoder_L9): Linear(in_features=512, out_features=13706, bias=True)\n",
            "  (decoder_R9): LeakyReLU(negative_slope=0.4, inplace=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=True)\n",
            ")\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ll3_lPCBM9y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define the optimization criterion / loss function\n",
        "loss_function = nn.BCEWithLogitsLoss(reduction='mean')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrXWJrZ4BM91",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define learning rate and optimization strategy\n",
        "learning_rate = 1e-3\n",
        "encoder_optimizer = torch.optim.Adam(encoder_train.parameters(), lr=learning_rate)\n",
        "decoder_optimizer = torch.optim.Adam(decoder_train.parameters(), lr=learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35QEjubbBM96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# specify training parameters\n",
        "num_epochs = 5\n",
        "mini_batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYKLFdZkBM9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert pre-processed data to pytorch tensor\n",
        "torch_dataset = torch.from_numpy(ori_subset_transformed.values).float()\n",
        "\n",
        "# convert to pytorch tensor - none cuda enabled\n",
        "dataloader = DataLoader(torch_dataset, batch_size=mini_batch_size, shuffle=True, num_workers=0)\n",
        "# note: we set num_workers to zero to retrieve deterministic results\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-UbCuPtBM-F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init collection of epoch losses\n",
        "epoch_losses = []\n",
        "\n",
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# train autoencoder model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # init collection of epoch losses\n",
        "    mini_batch_losses = []\n",
        "    \n",
        "    # init mini batch counter\n",
        "    mini_batch_count = 0\n",
        "    \n",
        "\n",
        "\n",
        "    # set networks in training mode (apply dropout when needed)\n",
        "    encoder_train.train()\n",
        "    decoder_train.train()\n",
        "\n",
        "    # start timer\n",
        "    start_time = datetime.now()\n",
        "        \n",
        "    # iterate over all mini-batches\n",
        "    for mini_batch_data in dataloader:\n",
        "\n",
        "        # increase mini batch counter\n",
        "        mini_batch_count += 1\n",
        "\n",
        "        # convert mini batch to torch variable\n",
        "        mini_batch_torch = autograd.Variable(mini_batch_data)\n",
        "\n",
        "        # =================== (1) forward pass ===================================\n",
        "\n",
        "        # run forward pass\n",
        "        z_representation = encoder_train(mini_batch_torch) # encode mini-batch data\n",
        "        mini_batch_reconstruction = decoder_train(z_representation) # decode mini-batch data\n",
        "        \n",
        "        # =================== (2) compute reconstruction loss ====================\n",
        "\n",
        "        # determine reconstruction loss\n",
        "        reconstruction_loss = loss_function(mini_batch_reconstruction, mini_batch_torch)\n",
        "        \n",
        "        # =================== (3) backward pass ==================================\n",
        "\n",
        "        # reset graph gradients\n",
        "        decoder_optimizer.zero_grad()\n",
        "        encoder_optimizer.zero_grad()\n",
        "\n",
        "        # run backward pass\n",
        "        reconstruction_loss.backward()\n",
        "        \n",
        "        # =================== (4) update model parameters ========================\n",
        "\n",
        "        # update network parameters\n",
        "        decoder_optimizer.step()\n",
        "        encoder_optimizer.step()\n",
        "\n",
        "        # =================== monitor training progress ==========================\n",
        "\n",
        "        # print training progress each 1'000 mini-batches\n",
        "        if mini_batch_count % 1000 == 0:\n",
        "            \n",
        "            # print the training mode: either on GPU or CPU\n",
        "            mode = 'GPU' if (torch.backends.cudnn.version() != None) and (USE_CUDA == True) else 'CPU'\n",
        "            \n",
        "            # print mini batch reconstuction results\n",
        "            now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "            end_time = datetime.now() - start_time\n",
        "            print('[LOG {}] training status, epoch: [{:04}/{:04}], batch: {:04}, loss: {}, mode: {}, time required: {}'.format(now, (epoch+1), num_epochs, mini_batch_count, np.round(reconstruction_loss.item(), 4), mode, end_time))\n",
        "\n",
        "            # reset timer\n",
        "            start_time = datetime.now()\n",
        "            \n",
        "        # collect mini-batch loss\n",
        "        mini_batch_losses.extend([np.round(reconstruction_loss.item(), 4)])\n",
        "\n",
        "    # =================== evaluate model performance =============================\n",
        "                                 \n",
        "    # collect mean training epoch loss\n",
        "    epoch_losses.extend([np.mean(mini_batch_losses)])\n",
        "    \n",
        "    # print training epoch results\n",
        "    now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "    print('[LOG {}] training status, epoch: [{:04}/{:04}], loss: {:.10f}'.format(now, (epoch+1), num_epochs, np.mean(mini_batch_losses)))\n",
        "\n",
        "    # =================== save model snapshot to disk ============================\n",
        "    \n",
        "    # save trained encoder model file to disk\n",
        "    encoder_model_name = \"ep_{}_encoder_model.pth\".format((epoch+1))\n",
        "    torch.save(encoder_train.state_dict(), encoder_model_name)\n",
        "\n",
        "    # save trained decoder model file to disk\n",
        "    decoder_model_name = \"ep_{}_decoder_model.pth\".format((epoch+1))\n",
        "    torch.save(decoder_train.state_dict(), decoder_model_name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjciNBx2BM-L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# plot the training progress\n",
        "plt.plot(range(0, len(epoch_losses)), epoch_losses)\n",
        "plt.xlabel('[training epoch]')\n",
        "plt.xlim([0, len(epoch_losses)])\n",
        "plt.ylabel('[reconstruction-error]')\n",
        "#plt.ylim([0.0, 1.0])\n",
        "plt.title('AENN training performance')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22eX7DGABM-P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init training network classes / architectures\n",
        "encoder_eval = encoder_train\n",
        "decoder_eval = decoder_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5vhpMJQBM-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert encoded transactional data to torch Variable\n",
        "data = autograd.Variable(torch_dataset)\n",
        "\n",
        "# set networks in evaluation mode (don't apply dropout)\n",
        "encoder_eval.eval()\n",
        "decoder_eval.eval()\n",
        "\n",
        "# reconstruct encoded transactional data\n",
        "reconstruction = decoder_eval(encoder_eval(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xOjPsYdBM-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H9ledlpEBM-a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# prepare plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# assign unique id to transactions\n",
        "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
        "\n",
        "# obtain regular transactions as well as global and local anomalies\n",
        "regular_data = plot_data[label == 'regular']\n",
        "global_outliers = plot_data[label == 'global']\n",
        "local_outliers = plot_data[label == 'local']\n",
        "\n",
        "# plot reconstruction error scatter plot\n",
        "ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", label='regular') # plot regular transactions\n",
        "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", label='global') # plot global outliers\n",
        "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"^\", label='local') # plot local outliers\n",
        "\n",
        "# add plot legend of transaction classes\n",
        "ax.legend(loc='best')\n",
        "\n",
        "print(np.mean(regular_data[:5, 1]))\n",
        "print(np.var(regular_data[:5, 1]))\n",
        "print(np.mean(global_outliers[:5, 1]))\n",
        "print(np.var(global_outliers[:5, 1]))\n",
        "print(np.mean(local_outliers[:5, 1]))\n",
        "print(np.var(local_outliers[:5, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XSQt13QqQYfQ",
        "colab": {}
      },
      "source": [
        "# convert encoded transactional data to torch Variable\n",
        "torch_test_dataset = torch.from_numpy(test_subset_transformed.values).float()\n",
        "data = autograd.Variable(torch_test_dataset)\n",
        "\n",
        "# set networks in evaluation mode (don't apply dropout)\n",
        "encoder_eval.eval()\n",
        "decoder_eval.eval()\n",
        "\n",
        "# reconstruct encoded transactional data\n",
        "reconstruction = decoder_eval(encoder_eval(data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "e439Zf-IQYfU",
        "colab": {}
      },
      "source": [
        "# init binary cross entropy errors\n",
        "reconstruction_loss_transaction = np.zeros(reconstruction.size()[0])\n",
        "\n",
        "# iterate over all detailed reconstructions\n",
        "for i in range(0, reconstruction.size()[0]):\n",
        "\n",
        "    # determine reconstruction loss - individual transactions\n",
        "    reconstruction_loss_transaction[i] = loss_function(reconstruction[i], data[i]).item()\n",
        "\n",
        "    if(i % 100000 == 0):\n",
        "\n",
        "        ### print conversion summary\n",
        "        now = datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
        "        print('[LOG {}] collected individual reconstruction loss of: {:06}/{:06} transactions'.format(now, i, reconstruction.size()[0]))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lbObnh12QYfZ",
        "colab": {}
      },
      "source": [
        "# prepare plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111)\n",
        "\n",
        "# assign unique id to transactions\n",
        "plot_data = np.column_stack((np.arange(len(reconstruction_loss_transaction)), reconstruction_loss_transaction))\n",
        "\n",
        "# obtain regular transactions as well as global and local anomalies\n",
        "regular_data = plot_data[test_label == 'regular']\n",
        "global_outliers = plot_data[test_label == 'global']\n",
        "local_outliers = plot_data[test_label == 'local']\n",
        "\n",
        "# plot reconstruction error scatter plot\n",
        "# ax.scatter(regular_data[:, 0], regular_data[:, 1], c='C0', alpha=0.4, marker=\"o\", label='regular') # plot regular transactions\n",
        "ax.scatter(global_outliers[:, 0], global_outliers[:, 1], c='C1', marker=\"^\", label='global') # plot global outliers\n",
        "ax.scatter(local_outliers[:, 0], local_outliers[:, 1], c='C2', marker=\"^\", label='local') # plot local outliers\n",
        "\n",
        "# add plot legend of transaction classes\n",
        "ax.legend(loc='best')\n",
        "\n",
        "print(np.mean(regular_data[:5, 1]))\n",
        "print(np.var(regular_data[:5, 1]))\n",
        "print(np.mean(global_outliers[:5, 1]))\n",
        "print(np.var(global_outliers[:5, 1]))\n",
        "print(np.mean(local_outliers[:5, 1]))\n",
        "print(np.var(local_outliers[:5, 1]))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}